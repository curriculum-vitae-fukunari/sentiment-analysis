{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# IMDbの映画レビューセットを用いた感情分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで扱う映画レビューデータセットは、「肯定的」か「否定的」かで分類される50000件の映画レビューで構成されている。  \n",
    "これらの映画レビューのサブセットから意味のある情報を抽出し、  \n",
    "レビューする人が映画を「好き」か「嫌い」のどちらを評価したのかを予測する機械学習モデルを構築する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import pandas\n",
    "from pandas.core.frame import DataFrame\n",
    "import os\n",
    "import numpy\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データを読み込み、一つのCSVファイルに出力する\n",
    "\n",
    "IMDb(Intenet Movie Database)から映画レビューデータセットを取り出す。  \n",
    "データは[こちら](http://ai.stanford.edu/~amaas/data/sentiment/)から取得した。  \n",
    "以下を実行する前に、./data/aclImdb_v1.tar.gzを解凍する必要がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12500/12500 [00:23<00:00, 531.25it/s]\n",
      "100%|██████████| 12500/12500 [00:32<00:00, 381.51it/s]\n",
      "100%|██████████| 12500/12500 [00:36<00:00, 345.78it/s]\n",
      "100%|██████████| 12500/12500 [00:45<00:00, 275.44it/s]\n"
     ]
    }
   ],
   "source": [
    "path_base: str = \"./data/aclImdb/\"\n",
    "path_test: str = path_base + \"test/\"\n",
    "path_train: str = path_base + \"train/\"\n",
    "neg: str = \"neg\"\n",
    "pos: str = \"pos\"\n",
    "path_test_negative: str = path_test + neg\n",
    "path_test_positive: str = path_test + pos\n",
    "path_train_negative: str = path_train + neg\n",
    "path_train_positive: str = path_train + pos\n",
    "\n",
    "df: DataFrame = pandas.DataFrame()\n",
    "\n",
    "\n",
    "def load_data(path: str, label: int, data_frame: DataFrame) -> DataFrame:\n",
    "    for file in tqdm(os.listdir(path)):\n",
    "        with open(os.path.join(path, file), 'r', encoding='utf-8') as infile:\n",
    "            text: str = infile.read()\n",
    "        data_frame = data_frame.append([[text, label]], ignore_index=True)\n",
    "    return data_frame\n",
    "\n",
    "\n",
    "df = load_data(path=path_test_negative, label=0, data_frame=df)\n",
    "df = load_data(path=path_test_positive, label=1, data_frame=df)\n",
    "df = load_data(path=path_train_negative, label=0, data_frame=df)\n",
    "df = load_data(path=path_train_positive, label=1, data_frame=df)\n",
    "\n",
    "df.columns = [\"review\", \"sentiment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データをシャッフルして、CSVファイルに出力する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.random.seed(0)\n",
    "df = df.reindex(numpy.random.permutation(df.index))\n",
    "df.to_csv(\"./data/movie_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テキストデータ内の単語を特徴ベクトルに変換する\n",
    "\n",
    "文章や単語などのカテゴリーデータは、機械学習アルゴリズムに渡す前に数値に変換する必要がある。  \n",
    "\n",
    "ここでは、テキストを数値の特徴ベクトルとして表現できるBoW(Bag of Words)モデルを用いる。  \n",
    "BoWモデルの基本的な考え方は以下のとおりである。  \n",
    "1. ドキュメントの集合全体から例えば単語という一意なトークン(token)からなる語彙を作成する。\n",
    "2. 各ドキュメントでの各単語の出現回数を含んだ特徴ベクトルを構築する。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### テキストデータをクレンジングする\n",
    "\n",
    "モデルを構築する前には、不要な文字を取り除く必要がある。  \n",
    "ここではHTMLマークアップ、顔文字以外の句読点を削除する。\n",
    "\n",
    "クレンジングの処理は以下のcleanse関数にて定義する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanse(text: str) -> str:\n",
    "    text = re.sub(\"<[^>]*>\", '', text)\n",
    "    emoticons: List[str] = re.findall(\"(?::|;|=)(?:-)?(?:\\)|\\(|D|P)\", text)\n",
    "    text = re.sub(\"[\\W]+]\", ' ', text.lower()) + ''.join(emoticons).replace(\"-\", '')\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ストップワードの除去を行う\n",
    "\n",
    "ストップワード(ドキュメントの判別に有効ではない単語、is, and, hasなど)の除去を行う。  \n",
    "レビューデータからストップワードを除去するには、NLTKライブラリで提供されている127個の英語のストップワードを使用する。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下では、nltkでストップワードのデータのダウンロードを行い、  \n",
    "pickeで英語に関するストップワードのデータをバイトコードにシリアライズする。  \n",
    "ここではすでにシリアライズ済みであるので、下記の実行の必要はない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/fukunaritakeshi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "\n",
    "path_pkl: str = os.path.join(\"./data/\", \"pkl_objects\")\n",
    "\n",
    "if not os.path.exists(path_pkl):\n",
    "    os.makedirs(path_pkl)\n",
    "    \n",
    "pickle.dump(stopwords.words(\"english\"),\n",
    "            open(os.path.join(path_pkl, \"stopwords.pkl\"), \"wb\"),\n",
    "            protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ストップワードのデシリアライズを行うには、下記を実行する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_reloaded: List[str] = pickle.load(open(os.path.join(path_pkl, \"stopwords.pkl\"), \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ステミングを行う\n",
    "\n",
    "トークン化にはワードステミング(Word Stemming)を用いる。  \n",
    "これは単語を原型に変換することで、関連する単語を同じ語幹にマッピングするようなプロセスである。  \n",
    "その中でもPorterステミングを用いる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上のクレンジング、ストップワードの除去、ステミングの一連の処理を以下のtokeninzer関数で定義する。  \n",
    "この関数は次節のHashingVectorizerにて使用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text: str):\n",
    "    cleanse(text)\n",
    "    porter_stemmer: PorterStemmer = PorterStemmer()\n",
    "\n",
    "    return [porter_stemmer.stem(word) for word in text.split()\n",
    "            if word not in stopwords_reloaded]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルをトレーニングする"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここではアウトオブコア学習+ミニバッチ学習を適用する。\n",
    "アウトオブコア学習とは、"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ベクタライザを用いる\n",
    "\n",
    "ここではHashingVectorizerを用いる。  \n",
    "(アウトオブコア学習でなければ、CountingVectorizerを用いることが可能である。)\n",
    "\n",
    "また、ここで前節のtokenizer関数を用いている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashing_vector: HashingVectorizer = HashingVectorizer(decode_error='ignore',\n",
    "                                                      n_features=2**21,\n",
    "                                                      preprocessor=None,\n",
    "                                                      tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ロジスティック回帰分類器を初期化する\n",
    "**SGDClassifierをなぜ使うのか**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_classifier: SGDClassifier = SGDClassifier(loss='log', random_state=1, max_iter=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ジェネレータ関数を定義する。\n",
    "  \n",
    "ドキュメントを１つずつ読み込んで返す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream(path: str) -> (str, int):\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        # ヘッダーを読み飛ばす\n",
    "        next(file)\n",
    "        for line in file:\n",
    "            text: str = line[:-3]\n",
    "            label: int = int(line[-2]) # 一行の最後尾は'\\n'のため\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ミニバッチ関数を定義する。\n",
    "\n",
    "stream_documents関数からドキュメントストリームを受け取り、  \n",
    "size引数によって指定された個数のドキュメントを返す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(documents_stream, size: int):\n",
    "    documents: List[str] = []\n",
    "    y: List[int] = []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(documents_stream)\n",
    "            documents.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        print(\"StopIteration\")\n",
    "        return None, None\n",
    "    return documents, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "アウトオブコア学習を行う。\n",
    "\n",
    "1000個ずつデータを読み込み、それを学習に用いる。  \n",
    "これを45回繰り返す(ミニバッチ法)。  \n",
    "つまり45000個を訓練データに用いて、残り5000個をテストデータとして用いる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes: numpy.ndarray = numpy.array([0, 1])\n",
    "document_stream = stream(path=\"./data/movie_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [02:46<00:00,  3.70s/it]\n"
     ]
    }
   ],
   "source": [
    "for index in tqdm(range(45)):\n",
    "    X_train, y_train = get_minibatch(documents_stream=document_stream, size=1000)\n",
    "\n",
    "    if not X_train:\n",
    "        print(\"breaked...\")\n",
    "        break\n",
    "\n",
    "    X_train_vectorized = hashing_vector.transform(X_train)\n",
    "    sgd_classifier.partial_fit(X=X_train_vectorized, y=y_train, classes=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習済みモデルに対して予測を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正解率:  0.845\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = get_minibatch(documents_stream=document_stream, size=5000)\n",
    "X_test = hashing_vector.transform(X_test)\n",
    "accuracy: float = sgd_classifier.score(X_test, y_test)\n",
    "\n",
    "print(\"正解率: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習済みの推定器に対して保存やリロードをする"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習済みの推定器を、バイトコードにシリアライズして保存する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(path_pkl):\n",
    "    os.makedirs(path_pkl)\n",
    "    \n",
    "pickle.dump(sgd_classifier,\n",
    "            open(os.path.join(path_pkl, \"classifier.pkl\"), \"wb\"),\n",
    "            protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習済みの推定器をデシリアライズする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_reloaded: SGDClassifier = pickle.load(open(os.path.join(path_pkl, \"classifier.pkl\"), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict:  positive\n",
      "Probability:  91.331173329\n"
     ]
    }
   ],
   "source": [
    "label: Dict[int, str] = {0: \"negative\", 1: \"positive\"}\n",
    "\n",
    "example: List[str] = [\"I love this movie\"]\n",
    "X = hashing_vector.transform(example)\n",
    "\n",
    "print(\"Predict: \", label[classifier_reloaded.predict(X)[0]])\n",
    "print(\"Probability: \", numpy.max(classifier_reloaded.predict_proba(X)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
